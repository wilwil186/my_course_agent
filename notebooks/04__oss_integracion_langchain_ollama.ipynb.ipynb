{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c0dc4d",
   "metadata": {},
   "source": [
    "# Clase 6 (OSS): Integración con **LangChain + Ollama**\n",
    "\n",
    "Notebook *100% open‑source*. Usa tu `.env` existente:\n",
    "\n",
    "```env\n",
    "MODEL=qwen2.5:7b-instruct\n",
    "OLLAMA_BASE_URL=http://localhost:11434\n",
    "TEMPERATURE=0.0\n",
    "```\n",
    "\n",
    "Requisitos previos:\n",
    "\n",
    "```bash\n",
    "uv add \"langchain>=0.3\" \"langchain-ollama>=0.3\" \"python-dotenv>=1.0\"\n",
    "uv add jupyter ipykernel --dev\n",
    "uv run python -m ipykernel install --user --name my-course-agent\n",
    "```\n",
    "\n",
    "Luego abre con:\n",
    "\n",
    "```bash\n",
    "uv run jupyter lab\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4a8fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e901512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "¡Hola! Estoy bien, gracias por preguntar. Soy un asistente de inteligencia artificial creado por Alibaba Cloud, así que no tengo sentimientos ni estados emocionales reales, pero estoy aquí para ayudarte en lo que necesites. ¿En qué puedo ser útil hoy?\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=os.getenv(\"MODEL\", \"qwen2.5:7b-instruct\"),\n",
    "    base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"),\n",
    "    temperature=float(os.getenv(\"TEMPERATURE\", \"0.0\")),\n",
    ")\n",
    "\n",
    "resp = llm.invoke(\"Hola, hola, ¿cómo estás?\")\n",
    "resp.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f41bd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Te llamas Wilson.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "msg_1 = SystemMessage(content=\"Eres un asistente útil que responde en español.\")\n",
    "msg_2 = HumanMessage(content=\"Me llamo Wilson.\")\n",
    "msg_3 = AIMessage(content=\"Hola Wilson, ¿cómo estás?\")\n",
    "msg_4 = HumanMessage(content=\"¿Cómo me llamo?\")\n",
    "history = [msg_1, msg_2, msg_3, msg_4]\n",
    "\n",
    "resp = llm.invoke(history)\n",
    "resp.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989dbc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Qwen2.5:7b ==\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Claro! Aquí tienes un pequeño truco: puedes usar f-strings para concatenar cadenas y variables de manera más limpia:\n",
      "\n",
      "```python\n",
      "nombre = \"Mariano\"\n",
      "print(f\"Bienvenido {nombre}!\")\n",
      "```\n",
      "\n",
      "Este código imprimirá: `Bienvenido Mariano!`\n",
      "== Llama3.1:8b ==\n",
      "Llama3.1:8b no disponible: model 'llama3.1:8b-instruct' not found (status code: 404)\n"
     ]
    }
   ],
   "source": [
    "def init_local_llm(model: str | None = None, temperature: float | None = None):\n",
    "    from langchain_ollama import ChatOllama\n",
    "    return ChatOllama(\n",
    "        model=model or os.getenv(\"MODEL\", \"qwen2.5:7b-instruct\"),\n",
    "        base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"),\n",
    "        temperature=float(temperature if temperature is not None else os.getenv(\"TEMPERATURE\", \"0.0\")),\n",
    "    )\n",
    "\n",
    "llm_qwen = init_local_llm(\"qwen2.5:7b-instruct\", temperature=0.0)\n",
    "models = [(\"Qwen2.5:7b\", llm_qwen)]\n",
    "\n",
    "try:\n",
    "    llm_llama = init_local_llm(\"llama3.1:8b-instruct\", temperature=0.0)\n",
    "    models.append((\"Llama3.1:8b\", llm_llama))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "for name, m in models:\n",
    "    try:\n",
    "        print(f\"== {name} ==\")\n",
    "        m.invoke(\"Dime un tip de Python en una línea.\").pretty_print()\n",
    "    except Exception as e:\n",
    "        print(f\"{name} no disponible: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94dac0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
