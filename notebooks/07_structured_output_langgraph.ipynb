{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0493da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explain_setup",
   "metadata": {},
   "source": [
    "## Configuración Inicial\n",
    "\n",
    "En esta celda, cargamos las variables de entorno desde un archivo `.env` (si existe). Esto permite configurar el modelo de Ollama y la URL del servidor de forma flexible, sin hardcodear valores en el código.\n",
    "\n",
    "- Usamos `load_dotenv()` para leer el archivo `.env`.\n",
    "- Definimos `MODEL` como el modelo de Ollama a usar (por defecto `llama3.1:8b` para mejor razonamiento).\n",
    "- `BASE_URL` apunta al servidor local de Ollama (por defecto `http://localhost:11434`).\n",
    "\n",
    "Luego, inicializamos el LLM con `ChatOllama`, que es una interfaz open source para modelos locales. Esto evita depender de APIs propietarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb8947f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola Nicolas! Encantado de conocerte. ¿En qué puedo ayudarte hoy? Si tienes alguna pregunta o necesitas asistencia con algo en particular, no dudes en decírmelo.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "MODEL = os.getenv(\"MODEL\", \"llama3.1:8b\")\n",
    "BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "\n",
    "llm = ChatOllama(model=MODEL, base_url=BASE_URL, temperature=0)\n",
    "response = llm.invoke('Hola soy Nicolas y mi telefono es +57 317 867 2615')\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explain_basic_invoke",
   "metadata": {},
   "source": [
    "## Invocación Básica del LLM\n",
    "\n",
    "Aquí invocamos el modelo de Ollama con un mensaje simple. El modelo responde de forma conversacional, procesando el input y devolviendo una respuesta en texto plano.\n",
    "\n",
    "- Usamos `ChatOllama` para conectar con el modelo local.\n",
    "- `invoke()` envía el mensaje y devuelve un objeto con `content` (la respuesta).\n",
    "- Esto demuestra el uso básico de LLMs locales, sin necesidad de APIs externas.\n",
    "\n",
    "Nota: La respuesta es en español, ya que el modelo (llama3.1:8b) es capaz de manejar múltiples idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2655c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nicolas'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    \"\"\"Contact information for a person.\"\"\"\n",
    "    name: Optional[str] = Field(description=\"The name of the person, if mentioned explicitly.\")\n",
    "    email: Optional[str] = Field(description=\"The email address of the person, if provided.\")\n",
    "    phone: Optional[str] = Field(description=\"The phone number of the person, if given.\")\n",
    "    tone: Optional[int] = Field(description=\"The tone of the person on a scale from 0 to 100, if inferable.\", ge=0, le=100)\n",
    "    age: Optional[int] = Field(description=\"The age of the person, if mentioned as a number.\")\n",
    "    sentiment: Optional[str] = Field(description=\"The sentiment of the conversation: positive, negative, or neutral, if inferable.\")\n",
    "\n",
    "llm_with_structured_output  = llm.with_structured_output(schema=ContactInfo)\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"Extrae información de contacto de la conversación. Si no encuentras un dato explícitamente, no lo inventes. Para sentimiento, infiere basado en el tono del mensaje: positivo, negativo o neutral. Usa null solo si no se puede inferir.\"),\n",
    "    (\"user\", \"Hola soy Nicolas y mi telefono es +57 317 867 2615\"),\n",
    "]\n",
    "\n",
    "response = llm_with_structured_output.invoke(messages)\n",
    "response.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explain_structured_output",
   "metadata": {},
   "source": [
    "## Salida Estructurada con Pydantic\n",
    "\n",
    "En esta sección, usamos `with_structured_output()` para forzar al LLM a devolver datos en un formato específico definido por un esquema Pydantic.\n",
    "\n",
    "- Definimos `ContactInfo` como un modelo Pydantic con campos opcionales (usando `Optional` para evitar errores si no se encuentra el dato).\n",
    "- El esquema incluye descripciones claras para guiar al modelo.\n",
    "- El prompt del sistema instruye al modelo a extraer información y, para sentimiento, inferir basado en el tono (positivo, negativo o neutral).\n",
    "- La respuesta es un objeto Pydantic, que podemos acceder como atributos (ej. `response.name`).\n",
    "\n",
    "Esto transforma respuestas impredecibles en datos estructurados, útiles para agentes que necesitan información confiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ffe3f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"name='Nicolas' email=None phone='+57 317 867 2615' tone=None age=None sentiment=None\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{response}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explain_result",
   "metadata": {},
   "source": [
    "## Análisis del Resultado\n",
    "\n",
    "El resultado muestra los datos extraídos:\n",
    "- `name='Nicolas'`: Extraído directamente del mensaje.\n",
    "- `phone='+57 317 867 2615'`: Teléfono proporcionado.\n",
    "- Otros campos como `email`, `tone`, `age` y `sentiment` son `None` porque no se mencionan explícitamente o no se pueden inferir con confianza.\n",
    "\n",
    "Esto demuestra cómo el structured output evita alucinaciones: el modelo no inventa datos que no están presentes. Para mejorar la inferencia de sentimiento, podríamos ajustar el prompt o usar un modelo más avanzado, pero este enfoque prioriza precisión sobre suposiciones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
