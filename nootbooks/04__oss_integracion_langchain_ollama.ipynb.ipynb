{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 6 (OSS): Integración con **LangChain + Ollama**\n",
    "\n",
    "Notebook *100% open‑source*. Usa tu `.env` existente:\n",
    "\n",
    "```env\n",
    "MODEL=qwen2.5:7b-instruct\n",
    "OLLAMA_BASE_URL=http://localhost:11434\n",
    "TEMPERATURE=0.0\n",
    "```\n",
    "\n",
    "Requisitos previos:\n",
    "\n",
    "```bash\n",
    "uv add \"langchain>=0.3\" \"langchain-ollama>=0.3\" \"python-dotenv>=1.0\"\n",
    "uv add jupyter ipykernel --dev\n",
    "uv run python -m ipykernel install --user --name my-course-agent\n",
    "```\n",
    "\n",
    "Luego abre con:\n",
    "\n",
    "```bash\n",
    "uv run jupyter lab\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=os.getenv(\"MODEL\", \"qwen2.5:7b-instruct\"),\n",
    "    base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"),\n",
    "    temperature=float(os.getenv(\"TEMPERATURE\", \"0.0\")),\n",
    ")\n",
    "\n",
    "resp = llm.invoke(\"Hola, hola, ¿cómo estás?\")\n",
    "resp.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "msg_1 = SystemMessage(content=\"Eres un asistente útil que responde en español.\")\n",
    "msg_2 = HumanMessage(content=\"Me llamo Juan.\")\n",
    "msg_3 = AIMessage(content=\"Hola Juan, ¿cómo estás?\")\n",
    "msg_4 = HumanMessage(content=\"¿Cómo me llamo?\")\n",
    "history = [msg_1, msg_2, msg_3, msg_4]\n",
    "\n",
    "resp = llm.invoke(history)\n",
    "resp.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_local_llm(model: str | None = None, temperature: float | None = None):\n",
    "    from langchain_ollama import ChatOllama\n",
    "    return ChatOllama(\n",
    "        model=model or os.getenv(\"MODEL\", \"qwen2.5:7b-instruct\"),\n",
    "        base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"),\n",
    "        temperature=float(temperature if temperature is not None else os.getenv(\"TEMPERATURE\", \"0.0\")),\n",
    "    )\n",
    "\n",
    "llm_qwen = init_local_llm(\"qwen2.5:7b-instruct\", temperature=0.0)\n",
    "models = [(\"Qwen2.5:7b\", llm_qwen)]\n",
    "\n",
    "try:\n",
    "    llm_llama = init_local_llm(\"llama3.1:8b-instruct\", temperature=0.0)\n",
    "    models.append((\"Llama3.1:8b\", llm_llama))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "for name, m in models:\n",
    "    try:\n",
    "        print(f\"== {name} ==\")\n",
    "        m.invoke(\"Dime un tip de Python en una línea.\").pretty_print()\n",
    "    except Exception as e:\n",
    "        print(f\"{name} no disponible: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-course-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}