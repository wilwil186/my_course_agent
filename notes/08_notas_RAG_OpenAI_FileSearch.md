# Clase 8 â€” RAG con OpenAI *File Search* (PDFs)

**Objetivo:** Implementar un RAG sencillo para consultar **PDFs** con la *tool* **file search** de OpenAI, sin montar tu propia base vectorial.

---

## ğŸ§  Â¿QuÃ© problema resuelve?
- Los LLM tienen una **ventana de conocimiento limitada** (no â€œsabenâ€ lo mÃ¡s reciente ni tu info privada).
- Con **RAG** puedes **adjuntar documentos** (p. ej., PDFs) y hacer que el modelo **razone con ese contexto** para responder con mÃ¡s precisiÃ³n.

---

## ğŸ”‘ Ideas clave
- **Vector stores**: almacenes de embeddings (OpenAI ofrece uno listo).
- **file search**: *tool* del proveedor que permite **buscar semÃ¡nticamente** dentro de tus PDFs.
- **Estrategia de contexto**: enviar **solo el Ãºltimo mensaje** al LLM para evitar errores por historiales largos (o resumir si necesitas memoria).

---

## âœ… Requisitos
- Cuenta y **API Key** de OpenAI.
- Proyecto creado en la plataforma.
- PDFs listos para subir.

---

## âš™ï¸ PreparaciÃ³n (Vector Store)
1. En **OpenAI â†’ Storage â†’ Vector stores**, crea uno (ej. `my-docs`).
2. **Sube** tus PDFs.
3. Copia el **`vector_store_id`** (lo usarÃ¡s desde el cÃ³digo).

---

## ğŸ§© InvocaciÃ³n con *file search*
- Define la *tool* con tus `vector_store_ids`.
- Instancia el LLM **de OpenAI** (esta *tool* es especÃ­fica del proveedor).
- EnvÃ­a **solo el Ãºltimo mensaje** del usuario al invocar.

```python
# IDs de tus vector stores
vector_store_ids = ["vs_XXXXXXXX"]

tools = [
    {"type": "file_search", "vector_store_ids": vector_store_ids}
]

# Ejemplo de invocaciÃ³n (esquemÃ¡tico)
llm = LLM(provider="openai", tools=tools)

last_message = history[-1].text  # o un resumen si necesitas memoria
response = llm.invoke(user_input=last_message)
print(response)
```

**Comportamiento esperado**
- Si la pregunta requiere contexto de PDF â†’ el modelo **usa la tool**.
- Si es trivial â†’ **responde directo** sin llamar a la tool.

---

## ğŸ› ï¸ IntegraciÃ³n rÃ¡pida en tu repo
1. **Crea** `src/agents/rag.py` (partiendo de tu `simple`):
   - Carga `.env`, instancia LLM de OpenAI con `tools=[file_search]`.
   - Recibe prompt y **pasa solo el Ãºltimo mensaje**.
2. **Registra** el agente en `langgraph.json` (ej. `"RAG": "./src/agents/rag.py:app"`).
3. **Reinicia** `langgraph dev` y prueba en la UI de chat.

> Tip: mantÃ©n **Qwen/Ollama** para tus agentes locales y usa **OpenAI** solo para este flujo de *file search*.

---

## ğŸ“ Buenas prÃ¡cticas / lÃ­mites
- **Prototipado rÃ¡pido**: ideal para *PoC* y *first use-cases* (p. ej., soporte con manuales PDF).
- **Menos personalizable** que un RAG propio (Chroma, PGVector, MongoDB+embeddings, etc.).
- Si necesitas **historial largo**, usa **resÃºmenes** o pasa a un **RAG mÃ¡s avanzado** (memoria + herramientas adicionales).

---

## ğŸ§ª Checklist de verificaciÃ³n
- [ ] Vector store creado y **PDFs subidos**  
- [ ] `vector_store_id` copiado  
- [ ] LLM de **OpenAI** configurado con *file search*  
- [ ] InvocaciÃ³n usando **solo el Ãºltimo mensaje**  
- [ ] Agente **registrado** en `langgraph.json` y probado en la UI  

---

## ğŸ”— Lecturas recomendadas
- DocumentaciÃ³n de **Agents** (OpenAI): https://platform.openai.com/docs/guides/agents

---

## ğŸ§­ PrÃ³ximos pasos (opcional)
- AÃ±adir **varios PDFs** y probar recuperaciÃ³n por secciones.
- Conectar **APIs externas** (ej., web search) para complementar contenido.
- Medir calidad: **fuentes citadas**, *chunking*, *reranking*.
