# Clase 8 ‚Äî RAG con OpenAI *File Search* (PDFs)

**Objetivo:** Implementar un RAG sencillo para consultar **PDFs** con la *tool* **file search** de OpenAI, sin montar tu propia base vectorial.

---

## üß† ¬øQu√© problema resuelve?
Los LLMs est√°ndar tienen limitaciones inherentes que RAG ayuda a superar.

- **Los LLM tienen una ventana de conocimiento limitada**: Est√°n entrenados hasta una fecha fija (ej. GPT-4 hasta 2023) y no acceden a datos privados o recientes. No "saben" sobre eventos actuales, documentos internos o conocimiento personalizado.
- **Con RAG (Retrieval-Augmented Generation) puedes adjuntar documentos (p. ej., PDFs)** y hacer que el modelo **razone con ese contexto** para responder con m√°s precisi√≥n, relevancia y actualizaci√≥n.
- **Beneficio clave**: Combina el poder generativo del LLM con informaci√≥n externa, evitando "alucinaciones" (respuestas inventadas) y mejorando exactitud en tareas como soporte t√©cnico, an√°lisis de docs o preguntas sobre datos privados.

RAG transforma un LLM gen√©rico en un experto en tu dominio espec√≠fico.

---

## üîë Ideas clave
Conceptos fundamentales para entender y usar RAG efectivamente.

- **Vector stores**: Almacenes de embeddings (representaciones vectoriales de texto) que permiten b√∫squedas sem√°nticas r√°pidas. OpenAI proporciona uno gestionado, pero puedes usar open-source como FAISS o Chroma para control total.
- **file search**: Herramienta espec√≠fica de OpenAI que integra b√∫squedas en vector stores con el LLM. Cuando el usuario pregunta, el modelo "busca" en tus documentos y usa resultados como contexto para responder.
- **Estrategia de contexto**: Para evitar costos altos y confusiones, env√≠a **solo el √∫ltimo mensaje** del usuario al LLM junto con resultados de b√∫squeda. Si necesitas memoria conversacional, resume historial o usa un sistema separado.
- **Flujo t√≠pico**: Usuario pregunta ‚Üí Buscar en vector store ‚Üí Inyectar resultados relevantes en prompt ‚Üí LLM responde con contexto.

Esta aproximaci√≥n es r√°pida para prototipos, pero para producci√≥n avanzada considera RAG propio con embeddings locales.

---

## ‚úÖ Requisitos
Antes de empezar, aseg√∫rate de tener estos elementos listos.

- **Cuenta y API Key de OpenAI**: Necesaria para usar file search. Crea cuenta en platform.openai.com si no tienes, y genera una API key en el dashboard (gu√°rdala en `.env` como `OPENAI_API_KEY`).
- **Proyecto creado en la plataforma de OpenAI**: Ve a platform.openai.com, crea un proyecto nuevo (gratuito para empezar) y copia el Project ID si lo necesitas (aunque para file search b√°sico no es esencial).
- **PDFs listos para subir**: Prepara documentos relevantes (ej. manuales, FAQs, reportes). Aseg√∫rate de que sean texto legible (no im√°genes escaneadas) y de tama√±o razonable (<100MB por archivo para l√≠mites iniciales).

Una vez listo, puedes subir PDFs directamente desde el c√≥digo o el dashboard.

---

## ‚öôÔ∏è Preparaci√≥n (Vector Store)
Configura el vector store en OpenAI para almacenar y buscar en tus documentos.

1. **Accede al dashboard**: Ve a platform.openai.com ‚Üí Storage ‚Üí Vector stores (o busca "Vector stores" en la barra lateral).
2. **Crea un vector store nuevo**: Haz clic en "Create vector store", n√≥mbralo (ej. `my-docs` para documentos del curso) y selecciona configuraci√≥n b√°sica (OpenAI maneja embeddings autom√°ticamente).
3. **Sube tus PDFs**: En el vector store creado, usa "Upload files" para subir documentos. OpenAI procesa y crea embeddings (puede tomar minutos para archivos grandes).
4. **Copia el `vector_store_id`**: Una vez subido, ve a los detalles del vector store y copia el ID (formato: `vs_xxxxxxxx`). Lo necesitar√°s en el c√≥digo para conectar.

- **Consejo**: Empieza con pocos PDFs para pruebas; agrega m√°s despu√©s.
- **L√≠mites**: Revisa quotas en tu plan (ej. almacenamiento gratis limitado).
- **Alternativa open-source**: Si prefieres control total, usa librer√≠as como LangChain + FAISS para vector stores locales (tema para clases futuras).

---

## üß© Invocaci√≥n con *file search*
Integra file search en tu LLM para b√∫squedas autom√°ticas en documentos.

### Definir la tool
```python
# IDs de tus vector stores (copia de dashboard)
vector_store_ids = ["vs_XXXXXXXX"]  # Reemplaza con tu ID real

# Definir tools para el LLM
tools = [
    {
        "type": "file_search",
        "vector_store_ids": vector_store_ids
    }
]

# Instancia LLM de OpenAI con tools
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
    model="gpt-4o-mini",  # O el modelo que uses
    tools=tools
)
```

### Invocar con contexto
```python
# Ejemplo de uso (sin grafo, directo)
from langchain_core.messages import HumanMessage

# Env√≠a solo el √∫ltimo mensaje (para simplicidad)
user_input = "Explica el concepto de RAG en base a los documentos subidos."
response = llm.invoke([HumanMessage(content=user_input)])
print(response.content)  # Respuesta con contexto de PDFs
```

- **Comportamiento esperado**:
  - Si la pregunta requiere contexto de PDF (ej. "Qu√© dice el doc sobre X?") ‚Üí El modelo llama a la tool autom√°ticamente, busca en vector store y usa resultados en respuesta.
  - Si es trivial (ej. "Hola") ‚Üí Responde directo sin tool, ahorrando costos.
- **Por qu√© solo √∫ltimo mensaje**: Evita tokens excesivos; si necesitas historial, resume previamente.

Esta integraci√≥n es autom√°tica: el LLM decide cu√°ndo usar la tool basado en la consulta.

---

## üõ†Ô∏è Integraci√≥n r√°pida en tu repo
A√±ade RAG a tu proyecto existente con pocos cambios.

1. **Crea archivo `src/agents/rag.py`** (basado en tu `main.py`):
   - Carga `.env` e instancia LLM de OpenAI con `tools=[file_search]`.
   - Funci√≥n que recibe texto, pasa solo el √∫ltimo mensaje al LLM y devuelve respuesta.
   ```python
   # Ejemplo m√≠nimo (adapta de tu main.py)
   from dotenv import load_dotenv
   from langchain_openai import ChatOpenAI
   from langchain_core.messages import HumanMessage

   load_dotenv()
   vector_store_ids = ["vs_XXXXXXXX"]  # Tu ID
   tools = [{"type": "file_search", "vector_store_ids": vector_store_ids}]
   llm = ChatOpenAI(model="gpt-4o-mini", tools=tools)

   def ask_rag(text: str) -> str:
       response = llm.invoke([HumanMessage(content=text)])
       return response.content

   # Para grafo (opcional)
   # ... (usa como nodo en StateGraph)
   ```

2. **Registra en `langgraph.json`** (agrega entrada):
   ```json
   {
     "dependencies": ["."],
     "graphs": {
       "agent": "./src/agents/main.py:app",
       "rag": "./src/agents/rag.py:ask_rag"  # O app si usas grafo
     },
     "env": ".env"
   }
   ```

3. **Reinicia y prueba en Studio**:
   - Corre `uv run langgraph dev`.
   - Selecciona el grafo "rag" en Studio.
   - Env√≠a preguntas como "Qu√© es LangGraph seg√∫n los docs?" y ve c√≥mo usa PDFs.

> Tip: Usa **Ollama** para agentes locales (gratis) y **OpenAI** solo para RAG con file search. Cambia `PROVIDER` en `.env` seg√∫n tarea.

---

## üìè Buenas pr√°cticas / l√≠mites
Maximiza efectividad y evita problemas comunes con RAG.

- **Prototipado r√°pido**: Ideal para PoCs y casos iniciales (ej. chatbot de soporte con manuales PDF). Sube docs, configura tool y listo.
- **Menos personalizable**: OpenAI maneja embeddings y b√∫squeda; no controlas detalles como chunking o reranking. Para customizaci√≥n, usa RAG propio (ej. LangChain + Chroma).
- **L√≠mites de contexto**: Si necesitas historial largo, usa res√∫menes (pide al LLM resumir conversaci√≥n) o integra con memoria externa. Evita enviar historial completo para reducir costos.
- **Otras pr√°cticas**:
  - **Chunking**: OpenAI chunk automaticamente; para control, preprocesa PDFs en texto peque√±o.
  - **Calidad**: Usa PDFs limpios (texto, no im√°genes); prueba b√∫squedas manuales en dashboard.
  - **Costos**: Monitorea uso (cada b√∫squeda consume tokens); optimiza con filtros o res√∫menes.
  - **Privacidad**: Datos en OpenAI (revisa pol√≠ticas); para sensible, usa vector stores locales.

Para producci√≥n avanzada, evoluciona a RAG h√≠brido (local + API).

---

## üß™ Checklist de verificaci√≥n
Aseg√∫rate de que RAG funcione correctamente antes de usar.

- [ ] Vector store creado en OpenAI dashboard y PDFs subidos/procesados.
- [ ] `vector_store_id` copiado y configurado en c√≥digo (ej. en `tools`).
- [ ] LLM de OpenAI instanciado con `tools=[file_search]` y probado directamente.
- [ ] Invocaci√≥n usando solo el √∫ltimo mensaje (o resumen si necesitas contexto).
- [ ] Agente registrado en `langgraph.json` (ej. nuevo grafo "rag") y probado en Studio (preguntas responden con contexto de PDFs).

Si todo est√° marcado, tienes RAG b√°sico funcionando.

---
## üîó Lecturas recomendadas
- **Documentaci√≥n de Agents (OpenAI)**: platform.openai.com/docs/guides/agents (gu√≠as oficiales para file search y tools).
- **LangChain RAG Guide**: python.langchain.com/docs/how_to/#qa-with-rag (para RAG avanzado open-source).
- **Vector Stores Open-Source**: github.com/facebookresearch/faiss (FAISS para b√∫squedas locales).

---
## üß≠ Pr√≥ximos pasos (opcional)
- **A√±adir varios PDFs**: Sube m√°s documentos y prueba recuperaci√≥n sem√°ntica (ej. preguntas sobre secciones espec√≠ficas).
- **Conectar APIs externas**: Integra web search (ej. Tavily) como tool adicional para info reciente.
- **Medir calidad**: Implementa m√©tricas como fuentes citadas, precisi√≥n de chunking y reranking para mejorar respuestas.
- **Evolucionar a RAG propio**: Usa LangChain + Chroma para control total (embeddings locales, filtros custom).

Este es un buen inicio; escala seg√∫n necesidades.
