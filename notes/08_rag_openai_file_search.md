# Clase 8 ‚Äî RAG con Herramientas Open Source (PDFs y Documentos)

**Objetivo:** Implementar un sistema RAG (Retrieval-Augmented Generation) completamente open source para consultar documentos como PDFs, usando herramientas locales como LangChain, FAISS y Ollama, sin depender de APIs propietarias. Esto permite control total, privacidad y cero costos variables.

---

## üß† ¬øQu√© problema resuelve?
Los LLMs est√°ndar tienen limitaciones inherentes que RAG ayuda a superar, pero usando herramientas open source mantenemos todo local y gratuito.

- **Los LLM tienen una ventana de conocimiento limitada**: Est√°n entrenados hasta una fecha fija y no acceden a datos privados o recientes. No "saben" sobre eventos actuales, documentos internos o conocimiento personalizado.
- **Con RAG open source puedes adjuntar documentos (p. ej., PDFs)** y hacer que el modelo **razone con ese contexto** para responder con m√°s precisi√≥n, relevancia y actualizaci√≥n, todo en tu m√°quina.
- **Beneficio clave**: Combina el poder generativo del LLM con informaci√≥n externa, evitando "alucinaciones" (respuestas inventadas) y mejorando exactitud en tareas como soporte t√©cnico, an√°lisis de docs o preguntas sobre datos privados. Adem√°s, es 100% privado y escalable sin costos.

RAG transforma un LLM gen√©rico en un experto en tu dominio espec√≠fico, usando solo software open source.

---

## üîë Ideas clave
Conceptos fundamentales para entender y usar RAG efectivamente con herramientas open source.

- **Vector stores locales**: Almacenes de embeddings (representaciones vectoriales de texto) que permiten b√∫squedas sem√°nticas r√°pidas. Usa FAISS (de Facebook) o Chroma para control total, corriendo en tu m√°quina.
- **Embeddings locales**: Calcula embeddings con modelos open source como `sentence-transformers` o Hugging Face, sin enviar datos a servidores externos.
- **Estrategia de contexto**: Para evitar costos altos y confusiones, env√≠a **solo el √∫ltimo mensaje** del usuario al LLM junto con resultados de b√∫squeda. Si necesitas memoria conversacional, resume historial o usa un sistema separado.
- **Flujo t√≠pico**: Usuario pregunta ‚Üí Cargar documentos ‚Üí Calcular embeddings ‚Üí Buscar en vector store ‚Üí Inyectar resultados relevantes en prompt ‚Üí LLM responde con contexto.

Esta aproximaci√≥n es ideal para privacidad y control; escala f√°cilmente sin l√≠mites de proveedores.

---

## ‚úÖ Requisitos
Antes de empezar, aseg√∫rate de tener estos elementos listos (todo open source y gratuito).

- **Ollama corriendo localmente**: Para el LLM (ej. `qwen2.5:7b-instruct`). Desc√°rgalo de ollama.ai e inicia con `ollama serve`.
- **PDFs listos para procesar**: Prepara documentos relevantes (ej. manuales, FAQs, reportes). Aseg√∫rate de que sean texto legible (no im√°genes escaneadas) y de tama√±o razonable.
- **Dependencias open source**: Instala con `uv add langchain langchain-community langchain-ollama faiss-cpu sentence-transformers pypdf` (para PDFs).

Una vez listo, procesa documentos localmente sin subir nada a servidores externos.

---

## ‚öôÔ∏è Preparaci√≥n (Vector Store Local)
Configura un vector store local con FAISS para almacenar y buscar en tus documentos de manera privada.

1. **Instala dependencias**:
   ```bash
   uv add langchain-community faiss-cpu sentence-transformers pypdf
   ```
   - `langchain-community`: Para loaders de documentos.
   - `faiss-cpu`: Vector store open source (usa CPU; para GPU instala `faiss-gpu`).
   - `sentence-transformers`: Modelos de embeddings gratuitos (ej. `all-MiniLM-L6-v2`).
   - `pypdf`: Para cargar PDFs.

2. **Carga y procesa documentos**:
   ```python
   from langchain_community.document_loaders import PyPDFLoader, TextLoader, DirectoryLoader
   from langchain_text_splitters import RecursiveCharacterTextSplitter

   # Carga PDFs de un directorio (ajusta path)
   loader = DirectoryLoader("PDF", glob="**/*.pdf", loader_cls=PyPDFLoader)
   docs = loader.load()

   # Divide en chunks (trozos) para mejor b√∫squeda
   splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
   chunks = splitter.split_documents(docs)
   print(f"Cargados {len(chunks)} chunks de documentos.")
   ```

3. **Calcula embeddings y crea vector store**:
   ```python
   from langchain_community.vectorstores import FAISS
   from langchain_huggingface import HuggingFaceEmbeddings

   # Modelo de embeddings open source (descarga autom√°ticamente)
   embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

   # Crea vector store local
   vectorstore = FAISS.from_documents(chunks, embeddings)
   vectorstore.save_local("faiss_index")  # Guarda en disco para reutilizar
   print("Vector store creado y guardado localmente.")
   ```

- **Consejo**: Empieza con pocos PDFs para pruebas; agrega m√°s despu√©s.
- **Ventajas open source**: Todo corre en tu m√°quina; no hay l√≠mites de almacenamiento ni costos.
- **Carga desde guardado**: Para reutilizar: `vectorstore = FAISS.load_local("faiss_index", embeddings)`.

---

## üß© Invocaci√≥n con RAG Local
Integra el vector store en tu LLM local para b√∫squedas autom√°ticas en documentos.

### Crear un retriever
```python
# Carga vector store (asumiendo ya creado)
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})  # Top 5 chunks relevantes

# Funci√≥n para buscar contexto
def get_context(query: str) -> str:
    results = retriever.get_relevant_documents(query)
    return "\n".join([doc.page_content for doc in results])
```

### Invocar LLM con contexto
```python
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, SystemMessage

# LLM local
llm = ChatOllama(model="qwen2.5:7b-instruct", temperature=0.3)

# Ejemplo de uso (sin grafo, directo)
user_input = "Explica el concepto de RAG en base a los documentos cargados."
context = get_context(user_input)
prompt = f"Contexto: {context}\nPregunta: {user_input}"
response = llm.invoke([HumanMessage(content=prompt)])
print(response.content)  # Respuesta con contexto de PDFs locales
```

- **Comportamiento esperado**:
  - Si la pregunta requiere contexto de PDF (ej. "Qu√© dice el doc sobre X?") ‚Üí Busca en vector store local y usa resultados en respuesta.
  - Si es trivial (ej. "Hola") ‚Üí Responde directo sin b√∫squeda, ahorrando recursos.
- **Por qu√© solo √∫ltimo mensaje**: Evita procesamiento excesivo; si necesitas historial, resume previamente.

Esta integraci√≥n es privada y r√°pida: todo en tu m√°quina.

---

## üõ†Ô∏è Integraci√≥n r√°pida en tu repo
A√±ade RAG a tu proyecto existente con pocos cambios, usando herramientas open source.

1. **Crea archivo `src/agents/rag.py`** (basado en tu `main.py`):
   - Carga documentos, crea vector store y funci√≥n para consultar.
   ```python
   # Ejemplo m√≠nimo (adapta de tu main.py)
   from dotenv import load_dotenv
   from langchain_ollama import ChatOllama
   from langchain_community.vectorstores import FAISS
   from langchain_huggingface import HuggingFaceEmbeddings
   from langchain_core.messages import HumanMessage

   load_dotenv()

   # Carga vector store (crea si no existe)
   embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
   vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

   retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
   llm = ChatOllama(model="qwen2.5:7b-instruct", temperature=0.3)

   def get_context(query: str) -> str:
       results = retriever.get_relevant_documents(query)
       return "\n".join([doc.page_content for doc in results])

   def ask_rag(text: str) -> str:
       context = get_context(text)
       prompt = f"Contexto: {context}\nPregunta: {text}"
       response = llm.invoke([HumanMessage(content=prompt)])
       return response.content

   # Para grafo (opcional)
   # ... (usa como nodo en StateGraph)
   ```

2. **Registra en `langgraph.json`** (agrega entrada):
   ```json
   {
     "dependencies": ["."],
     "graphs": {
       "agent": "./src/agents/main.py:app",
       "rag": "./src/agents/rag.py:ask_rag"  # O app si usas grafo
     },
     "env": ".env"
   }
   ```

3. **Reinicia y prueba en Studio**:
   - Corre `uv run langgraph dev`.
   - Selecciona el grafo "rag" en Studio.
   - Env√≠a preguntas como "Qu√© es LangGraph seg√∫n los docs?" y ve c√≥mo usa documentos locales.

> Tip: Usa **Ollama** para todo (LLM y embeddings) para mantenerlo 100% local y gratuito.

---

## üìè Buenas pr√°cticas / l√≠mites
Maximiza efectividad y evita problemas comunes con RAG open source.

- **Prototipado r√°pido y privado**: Ideal para casos sensibles (ej. documentos internos). Procesa localmente sin subir nada.
- **M√°s personalizable**: Controla embeddings, chunking y b√∫squeda; ajusta modelos seg√∫n necesidades.
- **L√≠mites de contexto**: Si necesitas historial largo, usa res√∫menes (pide al LLM resumir conversaci√≥n) o integra con memoria externa. Evita enviar historial completo para reducir recursos.
- **Otras pr√°cticas**:
  - **Chunking**: Usa `RecursiveCharacterTextSplitter` para dividir documentos en trozos relevantes (ajusta `chunk_size` y `chunk_overlap`).
  - **Calidad**: Usa documentos limpios (texto, no im√°genes); prueba b√∫squedas con `retriever.get_relevant_documents("query")`.
  - **Recursos**: Corre en CPU (m√°s lento pero gratis); para velocidad, usa GPU si disponible.
  - **Privacidad**: Todo local; no hay datos enviados a terceros.

Para producci√≥n avanzada, escala con m√°s documentos o integra b√∫squedas web open source.

---

## üß™ Checklist de verificaci√≥n
Aseg√∫rate de que RAG funcione correctamente antes de usar.

- [ ] Dependencias instaladas: `langchain-community`, `faiss-cpu`, `sentence-transformers`, `pypdf`.
- [ ] Documentos cargados y vector store creado/guardado localmente (ej. en `faiss_index`).
- [ ] Retriever configurado y probado directamente (busca contexto relevante).
- [ ] LLM local (Ollama) integrado con contexto de b√∫squeda.
- [ ] Invocaci√≥n usando contexto inyectado en prompt.
- [ ] Agente registrado en `langgraph.json` (ej. nuevo grafo "rag") y probado en Studio (preguntas responden con contexto de documentos).

Si todo est√° marcado, tienes RAG open source funcionando.

---

## üîó Lecturas recomendadas
- **LangChain RAG Guide**: python.langchain.com/docs/how_to/#qa-with-rag (para RAG avanzado open-source).
- **FAISS Documentation**: github.com/facebookresearch/faiss (vector store local).
- **Sentence Transformers**: huggingface.co/sentence-transformers (modelos de embeddings gratuitos).
- **Ollama para LLMs locales**: ollama.ai (ejecuta modelos open source en tu m√°quina).

---

## üß≠ Pr√≥ximos pasos (opcional)
- **A√±adir m√°s documentos**: Carga m√°s PDFs/TXT y reconstruye el vector store.
- **Mejorar b√∫squeda**: Integra reranking (ej. con `sentence-transformers` para reordenar resultados).
- **Conectar b√∫squedas web**: Usa herramientas open source como `langchain-community` para buscar en web (ej. DuckDuckGo).
- **Medir calidad**: Implementa m√©tricas como precisi√≥n de recuperaci√≥n y relevancia de respuestas.
- **Escalar**: Para grandes datasets, usa Chroma o Pinecone open source para vector stores m√°s avanzados.

Este es un buen inicio 100% open source; escala seg√∫n necesidades sin costos.
