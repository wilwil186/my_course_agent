# ‚öôÔ∏è Clase 2: Configuraci√≥n de entorno Python y tu primer agente (100% Open Source)

> Curso: **Crear Agentes de AI con LangGraph**  
> Objetivo: Dejar listo tu entorno con `uv` + `Ollama` + `LangGraph` (arquitectura `src/`) y levantar un agente m√≠nimo que podr√°s debuggear en **LangGraph Studio**, sin depender de APIs cerradas.

---

## üß© Qu√© aprender√°s
En esta clase, construir√°s una base s√≥lida para desarrollar agentes de IA de manera profesional y reproducible:

- **Crear y aislar el entorno con `uv`**: Aprender√°s a usar `uv` como alternativa moderna a `pip` y `virtualenv`, creando entornos virtuales aislados que evitan conflictos de dependencias y facilitan la colaboraci√≥n.
- **Instalar dependencias abiertas**: Configurar√°s paquetes clave como `langgraph` (para grafos), `langchain` (para integraci√≥n con LLMs) y `langchain-ollama` (para modelos locales), asegurando un stack 100% open-source.
- **Usar Ollama para correr modelos locales**: Instalar√°s y configurar√°s Ollama para ejecutar LLMs como `qwen2.5:7b-instruct` directamente en tu m√°quina, sin depender de APIs externas ni costos variables.
- **Exportar un grafo `app` y abrirlo en LangGraph Studio**: Crear√°s un agente m√≠nimo y lo visualizar√°s en Studio, una herramienta gr√°fica para depurar y probar grafos en tiempo real.
- **Buenas pr√°cticas con `.env` y arquitectura `src/`**: Gestionar√°s variables sensibles (como claves API) y organizar√°s el c√≥digo en una estructura modular para escalabilidad futura.

> Nota: Tu repo ya sigue este enfoque (estructura `src/`, `langgraph.json`, variables `MODEL` y `OLLAMA_BASE_URL`). Si sigues estas notas, todo encaja con lo que ya tienes. :contentReference[oaicite:1]{index=1}

---

## ‚úÖ Requisitos
Antes de empezar, aseg√∫rate de tener estos componentes b√°sicos instalados y configurados:

- **Python ‚â• 3.11** (recomendado 3.13): LangGraph requiere versiones modernas de Python para soporte completo de tipos y async. Puedes verificar con `python --version` o instalar desde python.org si es necesario.
- **Ollama corriendo localmente**: Una herramienta open-source para ejecutar LLMs en tu m√°quina. La usaremos para modelos como `qwen2.5:7b-instruct` sin necesidad de internet ni claves API. Desc√°rgala de ollama.ai e inicia con `ollama serve`.
- **uv instalado**: Un gestor de paquetes r√°pido y moderno que reemplaza a `pip` + `virtualenv`. Gestiona entornos virtuales, dependencias y locks de forma eficiente.

### Instalaci√≥n de `uv` (una sola vez por m√°quina)
Ejecuta estos comandos en tu terminal para instalar `uv` globalmente:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc
uv --version
```

Esto descarga el instalador oficial, lo ejecuta y agrega `uv` a tu PATH. El comando `uv --version` confirma que est√° listo. Si usas zsh u otro shell, ajusta el archivo correspondiente (ej. `~/.zshrc`).

> Nota: En tu README ya tienes estos pasos documentados con comandos √∫tiles para `uv`, incluyendo ejemplos de instalaci√≥n y uso b√°sico. Puedes referenciarlo para detalles adicionales. ([GitHub][1])

---

## üß± Crear el entorno y dependencias

Sigue estos pasos para configurar un entorno aislado y reproducible. Cada comando est√° explicado para que entiendas por qu√© es necesario.

1. **Crear entorno virtual con `uv`**:
   ```bash
   uv venv
   # (opcional) fijar versi√≥n exacta de Python si tienes m√∫ltiples instaladas
   # uv python pin 3.13
   ```
   - `uv venv` crea un directorio `.venv` con una instalaci√≥n aislada de Python y paquetes. Esto evita conflictos con instalaciones globales (ej. si tienes otros proyectos con versiones diferentes de las mismas librer√≠as).
   - El pin de Python asegura consistencia si tu sistema tiene varias versiones; om√≠telo si ya usas la recomendada.

2. **Agregar dependencias de producci√≥n**:
   ```bash
   uv add \
     "langgraph>=0.2" \
     "langchain>=0.3" \
     "langchain-core>=0.3" \
     "langchain-ollama>=0.3" \
     "pydantic<3" \
     "python-dotenv>=1.0"
   ```
   - Estas son las librer√≠as esenciales para el curso: `langgraph` para grafos, `langchain` para integraci√≥n con LLMs, `langchain-ollama` para modelos locales, `pydantic` para validaci√≥n de datos y `python-dotenv` para cargar variables de entorno.
   - `uv add` las agrega a `pyproject.toml` y las instala autom√°ticamente. Usa versiones m√≠nimas para compatibilidad, pero permite actualizaciones.

3. **Agregar dependencias de desarrollo** (para herramientas como Studio y Jupyter):
   ```bash
   uv add "langgraph-cli[inmem]" jupyter --dev
   ```
   - `langgraph-cli[inmem]` instala el comando `langgraph dev` para abrir LangGraph Studio sin necesidad de una base de datos externa.
   - `jupyter` permite notebooks para prototipado interactivo. El flag `--dev` las marca como opcionales para producci√≥n, manteniendo el entorno ligero.

4. **Sincronizar e instalar el paquete local (arquitectura `src/`)**:
   ```bash
   uv sync
   uv pip install -e .
   ```
   - `uv sync` instala todas las dependencias listadas en `pyproject.toml` y genera un `uv.lock` para reproducibilidad (similar a `poetry.lock`).
   - `uv pip install -e .` instala tu propio c√≥digo en modo "editable", permitiendo que imports como `from agents.main import app` funcionen sin reinstalar tras cambios. Es crucial para la arquitectura `src/` porque hace que el paquete sea importable como cualquier librer√≠a.

> Este flujo (sync + editable) es clave con `src/` para que m√≥dulos como `agents.main` se resuelvan correctamente. Lo documentaste ya en tu README con ejemplos pr√°cticos. ([GitHub][1])

---

## üîë Variables de entorno (`.env`)

Las variables de entorno permiten configurar el comportamiento de tu agente sin hardcodear valores sensibles o espec√≠ficos de ambiente. Crea un archivo `.env` en la ra√≠z del proyecto (y agr√©galo a `.gitignore` para evitar subir claves reales).

Ejemplo de contenido para `.env`:
```env
# Modelo local con Ollama (elige uno disponible en ollama pull)
MODEL=qwen2.5:7b-instruct
# URL donde Ollama escucha (por defecto en local)
OLLAMA_BASE_URL=http://localhost:11434
# Opcional: para tracing en LangSmith (desactiva si no usas)
# LANGSMITH_API_KEY=sk-...
# LANGCHAIN_TRACING_V2=false
```

- `MODEL`: Especifica el LLM a usar. Puedes cambiarlo a otros como `llama2:7b` o `mistral:7b` sin tocar c√≥digo.
- `OLLAMA_BASE_URL`: Apunta a la instancia de Ollama. Si corres en una m√°quina remota, c√°mbialo a `http://tu-ip:11434`.
- Comentarios: Usa `#` para notas; l√≠neas comentadas se ignoran.

Carga estas variables en tu c√≥digo con `from dotenv import load_dotenv; load_dotenv()` (ya incluido en los ejemplos).

> Esta configuraci√≥n coincide con tu repo actual y sigue las mejores pr√°cticas de Ollama/LangChain para modelos locales, evitando APIs cerradas. Consulta tu README para m√°s detalles y ejemplos. ([GitHub][1])

---

## üóÇÔ∏è Configuraci√≥n de LangGraph Studio (`langgraph.json`)

LangGraph Studio necesita saber d√≥nde encontrar tu grafo y qu√© entorno usar. Crea o edita `langgraph.json` en la ra√≠z del proyecto con esta estructura:

```json
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agents/main.py:app"
  },
  "env": ".env"
}
```

- `"dependencies": ["."]`: Indica que el proyecto actual es la dependencia principal (√∫til si tienes m√∫ltiples m√≥dulos).
- `"graphs"`: Mapea nombres de grafos a archivos. Aqu√≠, `"agent"` apunta a la variable `app` exportada en `src/agents/main.py`.
- `"env": ".env"`: Carga variables de entorno desde `.env` al iniciar Studio.

Este archivo act√∫a como un "√≠ndice" para Studio: cuando corres `uv run langgraph dev`, busca este JSON y carga el grafo para visualizaci√≥n y pruebas.

> Esta configuraci√≥n ya est√° implementada en tu repo exactamente como se muestra. Puedes verificar en tu `langgraph.json` y ajustarla si cambias la estructura de archivos. ([GitHub][1])

---

## ü§ñ C√≥digo base de tu primer agente (open-source)

Crea el archivo `src/agents/main.py` con este c√≥digo inicial. Es un agente m√≠nimo que responde a mensajes usando un LLM local v√≠a Ollama.

```python
from typing import TypedDict, List
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage
from langchain_ollama import ChatOllama
from langgraph.graph import StateGraph, MessagesState, START, END

# 1) Cargar variables de entorno desde .env
load_dotenv()

# 2) Instanciar el LLM local v√≠a Ollama (100% open source)
#    Usa las variables MODEL y OLLAMA_BASE_URL definidas en .env
llm = ChatOllama()

# 3) Definir un nodo que llama al modelo
def call_model(state: MessagesState):
    """
    Funci√≥n del nodo: recibe el estado (incluyendo lista de mensajes),
    invoca el LLM con esos mensajes y devuelve la respuesta agregada.
    """
    response = llm.invoke(state["messages"])
    return {"messages": [response]}

# 4) Construir el grafo de LangGraph
builder = StateGraph(MessagesState)
builder.add_node("model", call_model)
builder.add_edge(START, "model")
builder.add_edge("model", END)

# 5) Compilar el grafo en una aplicaci√≥n ejecutable
app = builder.compile()

# 6) Funci√≥n helper para invocar el agente desde c√≥digo o CLI
def ask(text: str) -> str:
    """
    Toma un texto de usuario, lo envuelve en mensajes (system + human),
    invoca el grafo y devuelve la respuesta del agente.
    """
    messages: List[BaseMessage] = [
        SystemMessage(content="Eres un asistente √∫til que contesta de forma breve y clara."),
        HumanMessage(content=text),
    ]
    result = app.invoke({"messages": messages})
    return result["messages"][-1].content
```

### Explicaci√≥n paso a paso del c√≥digo:
- **Imports**: `TypedDict` para tipos, mensajes de LangChain para estructurar conversaciones, `ChatOllama` para el LLM local, y componentes de LangGraph para el grafo.
- **Carga de .env**: `load_dotenv()` lee `MODEL` y `OLLAMA_BASE_URL` para configurar el LLM sin hardcodear.
- **Nodo `call_model`**: Es una funci√≥n pura que toma el estado (diccionario con mensajes), llama al LLM y devuelve cambios (solo la nueva respuesta). LangGraph maneja la fusi√≥n autom√°tica.
- **Grafo**: `StateGraph` define el flujo; `MessagesState` es un tipo predefinido para manejar listas de mensajes. `START` y `END` son puntos especiales.
- **Compilaci√≥n**: `app.compile()` convierte el builder en un objeto invocable.
- **Helper `ask`**: Facilita pruebas r√°pidas; construye mensajes iniciales y extrae la respuesta final.

* Este patr√≥n **MessagesState + un nodo** es el ‚Äúhola mundo‚Äù recomendado en la documentaci√≥n oficial de LangGraph. Es simple pero extensible: luego a√±adir√°s ramas, herramientas y memoria. ([LangChain Docs][2])
* `ChatOllama` es parte de `langchain-ollama` (paquete oficial) y conecta con cualquier modelo de Ollama, respetando tus variables de entorno. ([LangChain][3])

---

## ‚ñ∂Ô∏è Ejecutar y probar

Una vez configurado, prueba tu agente de dos formas: v√≠a c√≥digo/CLI para pruebas r√°pidas o en LangGraph Studio para depuraci√≥n visual.

**Desde Python/CLI** (pruebas r√°pidas):
```bash
uv run python -c "from agents.main import ask; print(ask('¬øCu√°l es el clima en Bogot√°?'))"
```
- `uv run` activa el entorno virtual y ejecuta el comando.
- Importa la funci√≥n `ask` y p√°sale un texto; deber√≠a responder usando el LLM local.
- Si falla (ej. "Module not found"), verifica que instalaste con `-e .` y que `src/agents/__init__.py` existe.

**Abrir LangGraph Studio** (modo visual para debug):
```bash
uv run langgraph dev
# Alternativa temporal (sin instalar globalmente):
# uvx langgraph dev
```
- Esto inicia un servidor local (por defecto en http://localhost:2024).
- Abre tu navegador y ver√°s el grafo `agent` cargado desde `langgraph.json`.
- Crea un "thread" nuevo, env√≠a mensajes y observa el flujo en tiempo real: c√≥mo se ejecuta cada nodo y c√≥mo fluye el estado.
- √ötil para depurar: si el agente no responde, inspecciona el estado en cada paso.

* Si Studio se queja de "no input", puedes a√±adir un nodo `ensure_input` que inyecte un mensaje por defecto como "Hola" (ya lo tienes documentado en tu README con ejemplos). ([GitHub][1])

---

## üß™ Verificaciones r√°pidas

Antes de proceder, confirma que todo est√© instalado y funcionando correctamente con estos comandos.

1. **Lista de paquetes instalados**:
   ```bash
   uv pip list
   ```
   - Deber√≠as ver `langgraph`, `langchain`, `langchain-ollama`, etc. en la lista.
   - Si falta algo, revisa que corriste `uv sync` y `uv add` correctamente.

2. **Probar el modelo en Ollama directamente**:
   ```bash
   ollama run qwen2.5:7b-instruct
   ```
   - Esto descarga el modelo si no lo tienes (puede tomar tiempo la primera vez) y abre una sesi√≥n interactiva.
   - Escribe "Hola, ¬øc√≥mo est√°s?" y presiona Enter; deber√≠a responder coherentemente.
   - Sal con Ctrl+D. Si falla, verifica que Ollama est√© corriendo (`ollama serve` en otra terminal).

Estas verificaciones aseguran que la integraci√≥n entre `uv`, LangGraph y Ollama funcione antes de a√±adir complejidad.

> Consulta la gu√≠a oficial de integraci√≥n con Ollama en LangChain para m√°s detalles sobre instalaci√≥n y uso avanzado. ([LangChain][4])

---

## üõ†Ô∏è Troubleshooting

Si encuentras errores comunes durante la configuraci√≥n, aqu√≠ van soluciones paso a paso:

* **`Package 'langgraph' does not provide any executables`** (al correr `langgraph dev`):
  - Soluci√≥n: Instala el CLI de desarrollo: `uv add "langgraph-cli[inmem]" --dev`.
  - Raz√≥n: El paquete base `langgraph` no incluye comandos; necesitas el extra `[inmem]` para Studio sin base de datos externa.
  - Referencia: Documentado en tu README con ejemplos. ([GitHub][1])

* **`ImportError: cannot import name 'app'`** (al importar desde `agents.main`):
  - Soluci√≥n: Aseg√∫rate de que `src/agents/main.py` exporte `app` (como en el c√≥digo arriba) y que `langgraph.json` apunte exactamente a `./src/agents/main.py:app`.
  - Raz√≥n: Python necesita que el m√≥dulo sea importable; verifica `__init__.py` en `src/agents/` y reinstala con `uv pip install -e .` si cambiaste archivos.
  - Referencia: Tu repo ya tiene esta configuraci√≥n; compara con el ejemplo. ([GitHub][1])

* **El modelo no existe en Ollama** (error al invocar LLM):
  - Soluci√≥n: Ejecuta `ollama pull qwen2.5:7b-instruct` (o el modelo en `MODEL`) para descargarlo. Opcionalmente, agrega `validate_model_on_init=True` al `ChatOllama` para verificar al inicio.
  - Raz√≥n: Ollama necesita el modelo descargado localmente; no lo descarga autom√°ticamente.
  - Referencia: Gu√≠a oficial de LangChain para integraci√≥n con Ollama. ([LangChain][5])

Si persiste un error, busca en GitHub Issues de LangGraph o pregunta en la comunidad con detalles del traceback.

---

## üìö Recursos abiertos

Estos recursos gratuitos y open-source te ayudar√°n a profundizar en los temas de esta clase:

* **LangGraph ‚Äì Overview & Graph API** (StateGraph, START/END, ejemplos b√°sicos y avanzados) ([LangChain Docs][2])
  - Explicaciones detalladas con c√≥digo de muestra para construir grafos desde cero.
* **LangChain + Ollama** (integraci√≥n oficial, instalaci√≥n y uso con modelos locales) ([LangChain][4])
  - Gu√≠a paso a paso para conectar Ollama con LangChain, incluyendo troubleshooting y mejores pr√°cticas.
* **uv Documentation** (gestor de paquetes y entornos) (busca "astral uv" en GitHub)
  - Tutoriales para proyectos Python, incluyendo grupos de dependencias y locks reproducibles.
* **Ollama GitHub** (modelos y configuraci√≥n) (github.com/ollama/ollama)
  - Lista de modelos disponibles, gu√≠as de instalaci√≥n y comunidad para soporte.

Usa estos para reforzar conceptos y resolver dudas espec√≠ficas sin depender de proveedores cerrados.

---

## ‚úÖ Checklist de esta clase

Usa esta lista para verificar que completaste todos los pasos antes de avanzar:

* [ ] Entorno virtual creado con `uv venv` y activado (verifica con `which python` apuntando a `.venv`).
* [ ] Dependencias instaladas correctamente (`langgraph`, `langchain`, `langchain-ollama`, `python-dotenv`, CLI dev con `jupyter`).
* [ ] Archivo `.env` creado con `MODEL=qwen2.5:7b-instruct` y `OLLAMA_BASE_URL=http://localhost:11434` (y cargado en c√≥digo).
* [ ] Archivo `src/agents/main.py` existe, exporta `app` y la funci√≥n `ask`, y se puede importar sin errores.
* [ ] Archivo `langgraph.json` apunta exactamente a `./src/agents/main.py:app` y carga `.env`.
* [ ] Comando `uv run langgraph dev` inicia Studio, carga el grafo `agent` y responde a mensajes de prueba.
* [ ] Modelo probado en Ollama directamente (`ollama run qwen2.5:7b-instruct` responde coherentemente).

Si todo est√° marcado, ¬°felicidades! Tienes un agente b√°sico funcionando con herramientas open-source.

**Siguiente clase ‚Üí** A√±adiremos **herramientas (tools)** al agente para que pueda realizar acciones externas, y introduciremos **branching** b√°sico en el grafo para decisiones condicionales.

