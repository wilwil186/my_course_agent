# üîå Clase 6: Integraci√≥n de **OpenAI** y **Anthropic** con LangChain (agn√≥stico y f√°cil de cambiar)

> Curso: **Crear Agentes de AI con LangGraph**  
> Objetivo: Conectar modelos de distintos proveedores sin reescribir el flujo. Mantener una **API homog√©nea** (con `invoke`), enviar **historial** (`System/Human/AI`) y alternar proveedor con una sola variable.

---

## üß† Arquitectura en una frase
Esta integraci√≥n crea agentes agn√≥sticos a proveedores, f√°ciles de cambiar y escalar.

- **LangGraph**: Orquesta el **flujo** del agente mediante grafos (nodos para acciones como razonamiento o tools, edges para transiciones, estado compartido para datos que fluyen entre nodos).
- **LangChain**: Estandariza la **conexi√≥n a modelos** (OpenAI, Anthropic, Ollama, etc.), proporcionando una API uniforme (`invoke`) para enviar mensajes y recibir respuestas, independientemente del proveedor.
- **T√∫ eliges el proveedor** por entorno/variable (ej. `PROVIDER=ollama` en `.env`); el grafo y la l√≥gica **no cambian**, permitiendo alternar entre modelos locales (open-source) y APIs cerradas sin reescribir c√≥digo.
- **Beneficio clave**: Portabilidad y flexibilidad; prueba con Ollama gratis, despliega con OpenAI si necesitas m√°s poder.

---

## ‚öôÔ∏è Instalaci√≥n (dependencias de producci√≥n)

Agrega estas librer√≠as a tu proyecto para conectar con proveedores populares. Usa `uv` para gesti√≥n limpia.

```bash
# Para proveedores cerrados (APIs pagas)
uv add langchain-openai langchain-anthropic

# (Opcional) Para open-source local (Ollama)
uv add langchain-ollama
```

- **langchain-openai**: Integra modelos de OpenAI (GPT-4, etc.) con API uniforme.
- **langchain-anthropic**: Para modelos de Anthropic (Claude) con misma interfaz.
- **langchain-ollama**: Para modelos locales como Qwen o Llama v√≠a Ollama (100% gratis y privado).
- **Por qu√© producci√≥n**: Estas son estables y usadas en entornos reales; instala solo las que necesites para mantener ligero.

Despu√©s de agregar, corre `uv sync` para instalar y actualizar `uv.lock`.

---

## üîë Variables de entorno (`.env` y `.env.example`)

Configura proveedores y modelos mediante variables de entorno para flexibilidad y seguridad.

### `.env.example` (plantilla p√∫blica, commitea esto)
```env
# Copia a .env y agrega tus claves reales

# OpenAI (API paga, requiere cuenta)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini   # Ej: gpt-4o, gpt-3.5-turbo

# Anthropic (API paga, alternativa a OpenAI)
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-5-sonnet-latest  # Ej: claude-3-opus-20240229

# Open-source local (Ollama, gratis)
PROVIDER=ollama            # Valores: "openai" | "anthropic" | "ollama"
MODEL=qwen2.5:7b-instruct  # Modelo en Ollama (ollama pull primero)
OLLAMA_BASE_URL=http://localhost:11434  # URL de Ollama (cambia si remoto)

# Hiperpar√°metros comunes (aplican a todos los proveedores)
TEMPERATURE=0.3  # Creatividad: 0.0=determinista, 1.0=creativo
```

### Carga en c√≥digo (agrega al inicio de tus archivos)
```python
import os
from dotenv import load_dotenv
load_dotenv()  # Lee .env y pone variables en os.environ

# Ejemplo de uso
provider = os.getenv("PROVIDER", "ollama")
model = os.getenv("MODEL", "qwen2.5:7b-instruct")
```

- **Seguridad**: Nunca commitees `.env` real (agrega a `.gitignore`). Usa `.env.example` para que otros sepan qu√© necesitan.
- **Flexibilidad**: Cambia proveedor/modelo editando `.env` sin tocar c√≥digo.
- **Hiperpar√°metros**: `TEMPERATURE` controla variabilidad; ajusta seg√∫n necesidades (bajo para tareas precisas, alto para creativas).

> **Buenas pr√°cticas**: nunca imprimas tus claves; usa `assert os.getenv("OPENAI_API_KEY") is not None` si quieres verificar.

---

## üßæ Tipos de mensajes (recordatorio r√°pido)

LangChain usa tipos espec√≠ficos para estructurar conversaciones, asegurando que el LLM entienda roles y contexto.

```python
from langchain_core.messages import (
    SystemMessage, HumanMessage, AIMessage, BaseMessage
)
```

- **SystemMessage**: Instrucciones del ‚Äúsistema‚Äù (estilo, reglas, contexto inicial). Ej: "Eres un asistente √∫til y respondes en espa√±ol." Invisible al usuario, pero gu√≠a el comportamiento.
- **HumanMessage**: Entrada de usuario. Ej: "Hola, ¬øc√≥mo est√°s?" Representa la consulta del humano.
- **AIMessage**: Respuesta del modelo. Ej: "¬°Hola! Estoy bien, ¬øen qu√© te ayudo?" Lo que el agente "dice".
- **BaseMessage**: Tipo base para listas mixtas. √ötil para tipar `list[BaseMessage]` sin especificar.

Estos tipos permiten enviar historial completo al LLM, manteniendo el contexto de la conversaci√≥n.

---

## ü§ù Enfoque **agn√≥stico** con un peque√±o inicializador

Crea un helper que selecciona proveedor y modelo basado en variables de entorno. Esto permite cambiar de proveedor (OpenAI, Anthropic, Ollama) sin modificar el resto del c√≥digo, manteniendo el grafo y l√≥gica intactos.

```python
import os
from dotenv import load_dotenv
from typing import Optional
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_ollama import ChatOllama  # Opcional para open-source

# Cargar .env al importar
load_dotenv()

def init_llm(
    provider: Optional[str] = None,
    model: Optional[str] = None,
    temperature: Optional[float] = None,
):
    """
    Inicializador agn√≥stico: elige proveedor basado en .env o par√°metros.
    Devuelve un LLM listo para usar con API uniforme (invoke).
    """
    provider = (provider or os.getenv("PROVIDER", "ollama")).lower()
    temperature = temperature if temperature is not None else float(os.getenv("TEMPERATURE", "0.3"))
    
    if provider == "openai":
        return ChatOpenAI(
            model=model or os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=temperature,
            # Opcional: api_key=os.getenv("OPENAI_API_KEY") si no usas default
        )
    elif provider == "anthropic":
        return ChatAnthropic(
            model=model or os.getenv("ANTHROPIC_MODEL", "claude-3-5-sonnet-latest"),
            temperature=temperature,
        )
    elif provider == "ollama":
        return ChatOllama(
            model=model or os.getenv("MODEL", "qwen2.5:7b-instruct"),
            base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
            temperature=temperature,
        )
    else:
        raise ValueError(f"Proveedor no soportado: {provider}. Usa 'openai', 'anthropic' o 'ollama'.")

# Uso: llm = init_llm()  # Toma de .env
# response = llm.invoke([HumanMessage(content="Hola")])
```

- **Agn√≥stico**: El mismo c√≥digo funciona con cualquier proveedor; cambia solo `.env`.
- **API uniforme**: Todos usan `invoke(mensajes)` y devuelven `AIMessage`.
- **Flexibilidad**: Pasa par√°metros para override (ej. `init_llm(provider="openai")`).
- **Error handling**: Lanza excepci√≥n clara si proveedor inv√°lido.

---

## üß© Enviar **historial** (system/human/ai) y mantenerlo en el estado

Env√≠a el historial completo al LLM para contexto, y acumula respuestas en el estado sin perder mensajes previos.

### Definir estado con historial
```python
from typing import TypedDict, Sequence
from typing_extensions import Annotated
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage
from langgraph.graph.message import add_messages

class State(TypedDict, total=False):
    messages: Annotated[Sequence[BaseMessage], add_messages]  # Acumula autom√°ticamente
    # Otras claves opcionales: customer_name, etc.
```

- `add_messages`: Agregador que appendea nuevos mensajes al historial existente.

### Nodo que usa historial y responde
```python
llm = init_llm()  # Inicializa con proveedor de .env

def llm_node(state: State) -> dict:
    """
    Nodo: Lee historial, agrega system prompt si falta,
    invoca LLM con contexto completo y devuelve nueva respuesta.
    """
    msgs = list(state.get("messages", []))  # Copia para no mutar

    # Asegura system prompt al inicio (si no existe)
    has_system = any(isinstance(m, SystemMessage) for m in msgs)
    if not has_system:
        msgs = [SystemMessage(content="Eres breve, claro y √∫til.")] + msgs

    # Invoca LLM con historial completo
    response = llm.invoke(msgs)  # Devuelve AIMessage

    # Devuelve solo lo nuevo (add_messages har√° append)
    return {"messages": [response]}

# Agrega a grafo
# builder.add_node("llm", llm_node)
```

- **Por qu√© funciona**: El LLM recibe todo el contexto (system + historial), permitiendo respuestas coherentes.
- **Acumulaci√≥n**: `add_messages` evita sobrescritura; cada invocaci√≥n agrega al final.

### Grafo m√≠nimo y prueba
```python
from langgraph.graph import StateGraph, START, END

builder = StateGraph(State)
builder.add_node("llm", llm_node)
builder.add_edge(START, "llm")
builder.add_edge("llm", END)
app = builder.compile()
```

Prueba en CLI:
```bash
uv run python -c "from agents.main import app; \
from langchain_core.messages import HumanMessage; \
result = app.invoke({'messages': [HumanMessage(content='Dime un tip r√°pido de Python')]}); \
print(result['messages'][-1].content)"
```

- Output ejemplo: "Usa list comprehensions para c√≥digo conciso: [x for x in range(10)]."
- Verifica acumulaci√≥n enviando m√∫ltiples mensajes en el mismo thread.

---

## üß™ Probar **cada proveedor** en segundos

Cambia el proveedor v√≠a variable de entorno y prueba inmediatamente. Aseg√∫rate de tener claves/API configuradas.

### OpenAI (API paga, requiere `OPENAI_API_KEY` en `.env`)
```bash
PROVIDER=openai uv run python -c "from agents.main import app; \
from langchain_core.messages import HumanMessage; \
result = app.invoke({'messages': [HumanMessage(content='Resume esto en 1 l√≠nea: List comprehensions')]}); \
print(result['messages'][-1].content)"
```
- Output ejemplo: "List comprehensions son una forma concisa de crear listas en Python usando una expresi√≥n."
- Nota: Consume cr√©ditos; usa para pruebas r√°pidas.

### Anthropic (API paga, alternativa, requiere `ANTHROPIC_API_KEY`)
```bash
PROVIDER=anthropic uv run python -c "from agents.main import app; \
from langchain_core.messages import HumanMessage; \
result = app.invoke({'messages': [HumanMessage(content='Explica ‚Äúdecorators‚Äù en Python en 1 l√≠nea')]}); \
print(result['messages'][-1].content)"
```
- Output ejemplo: "Decorators son funciones que modifican otras funciones agregando comportamiento antes o despu√©s de su ejecuci√≥n."
- Nota: Similar a OpenAI, pero con estilo propio (m√°s "pensativo").

### Ollama (local, open-source, gratis)
```bash
PROVIDER=ollama uv run python -c "from agents.main import app; \
from langchain_core.messages import HumanMessage; \
result = app.invoke({'messages': [HumanMessage(content='¬øQu√© es un grafo en 1 l√≠nea?')]}); \
print(result['messages'][-1].content)"
```
- Output ejemplo: "Un grafo es una estructura de datos compuesta por nodos conectados por aristas."
- Nota: Corre localmente; descarga modelo primero con `ollama pull qwen2.5:7b-instruct`.

Cambia `PROVIDER` en `.env` para hacer permanente el cambio sin comandos largos.

---

## üß™ Ejemplo directo (sin grafo) para validar instalaci√≥n

Prueba proveedores directamente (sin LangGraph) para validar que las librer√≠as funcionen.

### OpenAI
```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
resp = llm.invoke([HumanMessage(content="Hola, hola, ¬øc√≥mo est√°s?")])
print(type(resp))   # <class 'langchain_core.messages.ai.AIMessage'>
print(resp.content) # Ej: "¬°Hola! Estoy bien, gracias por preguntar. ¬øEn qu√© puedo ayudarte?"
```

### Anthropic
```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage

llm = ChatAnthropic(model="claude-3-5-sonnet-latest", temperature=0.3)
resp = llm.invoke([HumanMessage(content="Hola, hola, ¬øc√≥mo est√°s?")])
print(resp.content) # Ej: "Hola, estoy bien. ¬øY t√∫?"
```

### Ollama
```python
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage

llm = ChatOllama(model="qwen2.5:7b-instruct", base_url="http://localhost:11434", temperature=0.3)
resp = llm.invoke([HumanMessage(content="Hola, hola, ¬øc√≥mo est√°s?")])
print(resp.content) # Ej: "¬°Hola! Estoy aqu√≠ y listo para ayudar."
```

- **Cambio f√°cil**: Cambiar proveedor es solo cambiar el import y modelo; la API (`invoke`) es id√©ntica.
- **Validaci√≥n**: Si falla, verifica claves en `.env` o instalaci√≥n de Ollama.
- **Uso en grafo**: Usa el inicializador `init_llm()` para integrar en nodos.

---

## üéõÔ∏è Hiperpar√°metros y contexto
Ajusta estos par√°metros para controlar el comportamiento del LLM.

- **temperature**: Controla creatividad/variabilidad.
  - Bajo (0.0-0.3): Respuestas deterministas y precisas (ideal para hechos o c√≥digo).
  - Alto (0.7-1.0): M√°s creativo y diverso (bueno para brainstorming).
  - Ejemplo: `ChatOpenAI(temperature=0.1)` para respuestas consistentes.
- **top_p / top_k**: Filtros de sampling (no todos los proveedores lo soportan).
  - `top_p`: N√∫cleo de probabilidad (ej. 0.9 = usa tokens hasta cubrir 90% de probabilidad).
  - `top_k`: N√∫mero de tokens candidatos (ej. 50 = considera solo los 50 m√°s probables).
- **Contexto (historial)**: Env√≠a todo el historial o aplica estrategias.
  - Todo: M√°xima coherencia, pero costos altos.
  - Ventana: √öltimos N mensajes para reducir tokens.
  - Resumen: Pide al LLM resumir historial peri√≥dicamente.

Ajusta basado en tarea: precisi√≥n para soporte, creatividad para generaci√≥n.

---

## üõü Troubleshooting

Errores comunes al integrar proveedores y soluciones paso a paso.

- **`ValueError: Proveedor no soportado`**:
  - Causa: `PROVIDER` en `.env` no es "openai", "anthropic" o "ollama".
  - Soluci√≥n: Revisa `.env` y usa valores v√°lidos; prueba con `init_llm(provider="ollama")` para override.

- **`401/403` (Unauthorized/Forbidden)**:
  - Causa: Clave API inv√°lida o sin permisos.
  - Soluci√≥n: Verifica `OPENAI_API_KEY` o `ANTHROPIC_API_KEY` en `.env` (copia correcta de dashboard). Revisa l√≠mites de cuenta/plan.

- **`429 RateLimitError`**:
  - Causa: L√≠mite de cuota excedido (requests por minuto/hora).
  - Soluci√≥n: Espera, reduce concurrencia o ajusta plan/billing. Usa Ollama para pruebas ilimitadas.

- **`Model not found`**:
  - Causa: Nombre de modelo incorrecto en `.env`.
  - Soluci√≥n: Valida `OPENAI_MODEL` (ej. "gpt-4o-mini") o `ANTHROPIC_MODEL` (ej. "claude-3-5-sonnet-latest"). Lista modelos en dashboard.

- **Ollama no responde**:
  - Causa: Ollama no corriendo o URL incorrecta.
  - Soluci√≥n: Inicia `ollama serve` en terminal. Verifica `OLLAMA_BASE_URL` (default: http://localhost:11434). Descarga modelo con `ollama pull`.

- **Mensajes se borran** (historial perdido):
  - Causa: No usas `add_messages` o devuelves todo el estado.
  - Soluci√≥n: Define `messages` con `add_messages` en `State`; devuelve solo `{"messages": [nuevo]}`.

Prueba con ejemplos directos primero para aislar problemas de grafo.

---

## ‚úÖ Checklist

Verifica que la integraci√≥n funcione correctamente antes de avanzar.

- [ ] Dependencias instaladas: `langchain-openai` y/o `langchain-anthropic` (y `langchain-ollama` para local).
- [ ] Variables en `.env` configuradas (`OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `PROVIDER`, modelos, `TEMPERATURE`).
- [ ] Helper `init_llm` implementado y probado; `invoke` devuelve `AIMessage` con contenido v√°lido.
- [ ] Grafo m√≠nimo integrado (nodo con `llm_node`) y probado en CLI (respuestas coherentes).
- [ ] Probado en LangGraph Studio (cambia `PROVIDER` y verifica respuestas).
- [ ] Historial en estado con `add_messages` (mensajes se acumulan sin borrarse).
- [ ] Ejemplos directos funcionando para cada proveedor (sin grafo).

Si todo est√° marcado, puedes cambiar proveedores f√°cilmente.

**Siguiente clase ‚Üí** Branching y tools: c√≥mo a√±adir decisiones condicionales (ramas) y herramientas externas (b√∫squeda, c√°lculo, I/O) para agentes m√°s inteligentes.
