# üîå Clase 6: Integraci√≥n de **OpenAI** y **Anthropic** con LangChain (agn√≥stico y f√°cil de cambiar)

> Curso: **Crear Agentes de AI con LangGraph**  
> Objetivo: Conectar modelos de distintos proveedores sin reescribir el flujo. Mantener una **API homog√©nea** (con `invoke`), enviar **historial** (`System/Human/AI`) y alternar proveedor con una sola variable.

---

## üß† Arquitectura en una frase
- **LangGraph**: orquesta el **flujo** (grafos, estado compartido, branching).  
- **LangChain**: estandariza la **conexi√≥n a modelos** (OpenAI, Anthropic, Ollama, etc.).  
- **T√∫** eliges el **proveedor** por entorno/variable; el grafo y la l√≥gica **no cambian**.

---

## ‚öôÔ∏è Instalaci√≥n (dependencias de producci√≥n)

```bash
uv add langchain-openai langchain-anthropic
# (opcional, para seguir 100% open-source tambi√©n):
uv add langchain-ollama
```

---

## üîë Variables de entorno (`.env` y `.env.example`)

`.env.example` (no subas claves reales):
```env
# OpenAI
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini   # o el que prefieras

# Anthropic
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-5-sonnet-latest

# Open-source (Ollama)
PROVIDER=ollama            # "openai" | "anthropic" | "ollama"
MODEL=qwen2.5:7b-instruct
OLLAMA_BASE_URL=http://localhost:11434

# Hiperpar√°metros comunes
TEMPERATURE=0.3
```

Carga las variables en tu c√≥digo:
```python
import os
from dotenv import load_dotenv
load_dotenv()
```

> **Buenas pr√°cticas**: nunca imprimas tus claves; usa `assert os.getenv("OPENAI_API_KEY") is not None` si quieres verificar.

---

## üßæ Tipos de mensajes (recordatorio r√°pido)

```python
from langchain_core.messages import (
    SystemMessage, HumanMessage, AIMessage, BaseMessage
)
```

- `SystemMessage`: instrucciones del ‚Äúsistema‚Äù (estilo, reglas).  
- `HumanMessage`: entrada de usuario.  
- `AIMessage`: respuesta del modelo.  
- `BaseMessage`: tipo base √∫til para listas heterog√©neas.

---

## ü§ù Enfoque **agn√≥stico** con un peque√±o inicializador

Crea un helper que **selecciona proveedor y modelo** usando variables de entorno. As√≠ cambias de OpenAI ‚áÑ Anthropic ‚áÑ Ollama sin tocar el grafo.

```python
import os
from dotenv import load_dotenv
from typing import Optional
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_ollama import ChatOllama  # opcional si usas modelos locales

load_dotenv()

def init_llm(
    provider: Optional[str] = None,
    model: Optional[str] = None,
    temperature: Optional[float] = None,
):
    provider = (provider or os.getenv("PROVIDER", "ollama")).lower()
    temperature = temperature if temperature is not None else float(os.getenv("TEMPERATURE", "0.3"))
    if provider == "openai":
        return ChatOpenAI(
            model=model or os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=temperature,
        )
    elif provider == "anthropic":
        return ChatAnthropic(
            model=model or os.getenv("ANTHROPIC_MODEL", "claude-3-5-sonnet-latest"),
            temperature=temperature,
        )
    elif provider == "ollama":
        return ChatOllama(
            model=model or os.getenv("MODEL", "qwen2.5:7b-instruct"),
            base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
            temperature=temperature,
        )
    else:
        raise ValueError(f"Proveedor no soportado: {provider}")
```

---

## üß© Enviar **historial** (system/human/ai) y mantenerlo en el estado

```python
from typing import TypedDict, Sequence
from typing_extensions import Annotated
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage
from langgraph.graph.message import add_messages

class State(TypedDict, total=False):
    messages: Annotated[Sequence[BaseMessage], add_messages]  # se concatena
```

Nodo que usa el historial y **agrega** la nueva respuesta (no pisa lo anterior):

```python
llm = init_llm()  # toma PROVIDER/MODELO/TEMPERATURE desde .env

def llm_node(state: State) -> dict:
    msgs = list(state.get("messages", []))

    # Garantiza que exista un system prompt al inicio
    has_system = any(isinstance(m, SystemMessage) for m in msgs)
    if not has_system:
        msgs = [SystemMessage(content="Eres breve, claro y √∫til.")] + msgs

    # Invoca el modelo con todo el historial
    response = llm.invoke(msgs)  # -> AIMessage

    # Devuelve SOLO lo nuevo (messages stage har√° append)
    return {"messages": [response]}
```

Orquestaci√≥n m√≠nima:

```python
from langgraph.graph import StateGraph, START, END

builder = StateGraph(State)
builder.add_node("llm", llm_node)
builder.add_edge(START, "llm")
builder.add_edge("llm", END)
app = builder.compile()
```

Prueba r√°pida (CLI):
```bash
uv run python -c "from agents.main import app; \
from langchain_core.messages import HumanMessage; \
print(app.invoke({'messages':[HumanMessage(content='Dime un tip r√°pido de Python')]} )['messages'][-1].content)"
```

---

## üß™ Probar **cada proveedor** en segundos

**OpenAI** (requiere `OPENAI_API_KEY` y `OPENAI_MODEL`):
```bash
PROVIDER=openai uv run python -c "from agents.main import app; \
from langchain_core.messages import HumanMessage; \
print(app.invoke({'messages':[HumanMessage(content='Resume esto en 1 l√≠nea: List comprehensions')]} )['messages'][-1].content)"
```

**Anthropic** (requiere `ANTHROPIC_API_KEY` y `ANTHROPIC_MODEL`):
```bash
PROVIDER=anthropic uv run python -c "from agents.main import app; \
from langchain_core.messages import HumanMessage; \
print(app.invoke({'messages':[HumanMessage(content='Explica ‚Äúdecorators‚Äù en Python en 1 l√≠nea')]} )['messages'][-1].content)"
```

**Ollama** (local, open‚Äësource):
```bash
PROVIDER=ollama uv run python -c "from agents.main import app; \
from langchain_core.messages import HumanMessage; \
print(app.invoke({'messages':[HumanMessage(content='¬øQu√© es un grafo en 1 l√≠nea?')]} )['messages'][-1].content)"
```

---

## üß™ Ejemplo directo (sin grafo) para validar instalaci√≥n

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
resp = llm.invoke([HumanMessage(content="Hola, hola, ¬øc√≥mo est√°s?")])
print(type(resp))   # AIMessage
print(resp.content) # texto
```

Cambiar de modelo es **cambiar una cadena**. Con Anthropic:
```python
from langchain_anthropic import ChatAnthropic
llm = ChatAnthropic(model="claude-3-5-sonnet-latest", temperature=0.3)
```

---

## üéõÔ∏è Hiperpar√°metros y contexto
- **temperature**: creatividad (alto = m√°s creativo).  
- **top_p / top_k**: algunos proveedores lo exponen adicionalmente.  
- **historial**: env√≠a todo o aplica ventana/sumario seg√∫n costos y necesidades.

---

## üõü Troubleshooting

- **`ValueError: Proveedor no soportado`** ‚Üí revisa `PROVIDER` en `.env`.  
- **`401/403`** ‚Üí clave inv√°lida o falta de permisos (revisa cuenta/plan).  
- **`429 RateLimit`** ‚Üí l√≠mite de cuota; espera, baja concurrencia o ajusta plan.  
- **`Model not found`** ‚Üí nombre incorrecto; valida `OPENAI_MODEL` / `ANTHROPIC_MODEL`.  
- **Ollama no responde** ‚Üí verifica `OLLAMA_BASE_URL` y que `ollama serve` est√© activo.  
- **Mensajes se borran** ‚Üí define `messages` con `add_messages` y devuelve solo lo nuevo.

---

## ‚úÖ Checklist
- [ ] Instalaste `langchain-openai` y/o `langchain-anthropic` (y `langchain-ollama` si usas local).  
- [ ] Variables en `.env` (`OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `PROVIDER`, modelos‚Ä¶).  
- [ ] Helper `init_llm` funcionando; `invoke` devuelve `AIMessage`.  
- [ ] Grafo m√≠nimo integrado y probado en CLI/Studio.  
- [ ] Historial en el estado con `add_messages` (no se pisa).

**Siguiente clase ‚Üí** branching y tools: decisiones autom√°ticas y acciones (b√∫squeda, c√°lculo, I/O).
