# ==============================
# Configuración del modelo local
# ==============================

# Modelo de Ollama a usar (puedes cambiar por deepseek-r1:7b, llama3.1:8b, etc.)
MODEL=qwen2.5:7b-instruct

# Dirección del servidor Ollama local (no cambiar a menos que lo muevas de puerto)
OLLAMA_BASE_URL=http://localhost:11434

# ==============================
# Configuración opcional (LangGraph / LangSmith)
# ==============================

# No es necesaria, pero si quieres evitar el aviso naranja en Studio:
# LANGSMITH_API_KEY=<deja vacío o comenta esta línea>
# LANGCHAIN_TRACING_V2=false
